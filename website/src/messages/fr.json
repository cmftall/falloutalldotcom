{
  "navigation": {
    "home": "Accueil",
    "work": "Réalisations",
    "faq": "FAQ",
    "contact": "Contact",
    "hireMeCta": "M'engager",
    "caseStudies": "Réalisations"
  },
  "common": {
    "toggleTheme": "Basculer le thème"
  },
  "error": {
    "title": "Une erreur est survenue",
    "message": "Nous sommes désolés, mais quelque chose d'inattendu s'est produit. Veuillez essayer d'actualiser la page.",
    "details": "Détails de l'erreur",
    "tryAgain": "Réessayer",
    "refreshPage": "Actualiser la page"
  },
  "hero": {
    "headline": "Je répare les systèmes de données qui vous coûtent de l'argent",
    "subheadline": "Architecte de Données Consultant | Plus de €200K de valeur annuelle livrés | Disponible pour des projets de 2 à 6 mois",
    "primaryCta": "Réserver un Appel Stratégique Gratuit",
    "secondaryCta": "Voir les Études de Cas",
    "credential": "Architecte de Données Consultant • Paris et Montréal",
    "availability": "Disponible immédiatement pour nouveaux projets",
    "projectOutcomes": "30% de réduction d'erreurs • Plus de €200K de valeur annuelle • Plus de 100 pipelines en production",
    "technicalInnovation": "Des solutions durables que les équipes maintiennent et étendent avec succès, livrant des résultats business mesurables",
      "credibilityIndicators": [
        "Direction d'équipes de 7+ ingénieurs",
        "Plus de €200K de valeur annuelle grâce à l'innovation en qualité des données et architecture",
        "Plus de 100 pipelines de production construits et maintenus",
        "Expertise multi-industrie (Banque, Télécom, Technologie)",
        "Plus de 8 ans de leadership en transformation d'entreprise"
      ],
    "professionalPositioning": "Je me concentre sur la valeur business - livrer du ROI grâce à une architecture éprouvée qui priorise ce qui compte : réduire les coûts, améliorer la qualité et construire des équipes performantes",
    "savingsCalculation": "Plus de €200K calculés : 30% de réduction d'erreurs (plus de €65K) + 25% de gain d'efficacité (plus de €65K) + 40% de vitesse des tests (plus de €20K) + valeur de la plateforme business = plus de €200K de valeur annuelle"
  },
  "expertise": {
    "title": "Expertise",
    "subtitle": "Compétences clés organisées par niveau de maîtrise et profondeur d'expérience",
    "tierDescription": {
      "core": "Domaines d'expertise approfondie acquise à travers 6-8+ ans de pratique professionnelle",
      "advanced": "Maîtrise solide avec 4-5 ans d'expérience pratique",
      "working": "Connaissance pratique et développement continu"
    },
    "technicalDepth": "Compétences techniques avancées et méthodologies en architecture de données",
    "architecturalThinking": "Planification stratégique d'architecture de données et solutions à échelle d'entreprise",
    "methodologies": [
      "Frameworks d'automatisation DataOps",
      "Architecture de données micro-services",
      "Stratégies de migration sans interruption",
      "Déploiement avancé de modèles ML",
      "Gouvernance de données multi-cloud"
    ],
    "tiers": {
      "core": {
        "title": "Expertise Principale",
        "description": "Expertise approfondie prouvée par une pratique professionnelle extensive",
        "categories": {
          "dataEngineering": {
            "name": "Ingénierie des Données",
            "skills": "Python, Scala, SQL, Apache Airflow, Terraform",
            "proficiency": "Expert",
            "yearsExperience": 8,
            "methodologies": ["Conception de pipelines ETL/ELT", "Cadres de qualité de données", "Infrastructure as Code"]
          },
          "databases": {
            "name": "Bases de Données",
            "skills": "PostgreSQL, Oracle, Cassandra, MongoDB, Delta Lake",
            "proficiency": "Expert",
            "yearsExperience": 7,
            "methodologies": ["Optimisation de bases de données", "Modélisation de données", "Optimisation de performance"]
          },
          "cloudPlatforms": {
            "name": "Architecture de Données Cloud",
            "skills": "Azure, AWS, Databricks, Snowflake, Apache Spark",
            "proficiency": "Expert",
            "yearsExperience": 6,
            "methodologies": ["Architecture multi-cloud", "Traitement de données cloud-native", "Pipelines de données serverless"]
          }
        }
      },
      "advanced": {
        "title": "Compétences Avancées",
        "description": "Maîtrise solide avec expérience de livraison prouvée",
        "categories": {
          "machineLearning": {
            "name": "Apprentissage Automatique",
            "skills": "Scikit-Learn, TensorFlow, Pandas, NumPy, MLflow",
            "proficiency": "Avancé",
            "yearsExperience": 5,
            "methodologies": ["Déploiement de modèles ML", "Feature engineering", "Monitoring de modèles"]
          },
          "devops": {
            "name": "DevOps & DataOps",
            "skills": "GitHub Actions, Azure DevOps, Docker, Kubernetes, DataOps",
            "proficiency": "Avancé",
            "yearsExperience": 5,
            "methodologies": ["Pipelines CI/CD", "Orchestration de conteneurs", "Automatisation DataOps"]
          },
          "visualization": {
            "name": "Visualisation de Données",
            "skills": "Tableau, Power BI, Jupyter, Streamlit, Plotly",
            "proficiency": "Avancé",
            "yearsExperience": 4,
            "methodologies": ["Tableaux de bord interactifs", "Data storytelling", "Analytique en temps réel"]
          }
        }
      }
    }
  },
  "education": {
    "title": "Formation",
    "subtitle": "Fondation en sciences mathématiques avec spécialisation Big Data",
    "degrees": {
      "aims-masters-bigdata": {
        "degree": "Master en Sciences Mathématiques - Big Data",
        "institution": "AIMS",
        "year": "2018"
      },
      "ucad-masters-applied": {
        "degree": "Master en Mathématiques Appliquées",
        "institution": "Université Cheikh Anta Diop",
        "year": "2016"
      }
    }
  },
  "certifications": {
    "title": "Certifications et Titres",
    "subtitle": "Certifications professionnelles qui valident mon expertise technique et mon engagement envers l'apprentissage continu.",
    "viewDetails": "Voir les détails"
  },
  "contact": {
    "title": "Prêt à Livrer un ROI Mesurable ?",
    "subtitle": "Prêt à résoudre vos problèmes de données ? Réservez un appel stratégique gratuit de 30 minutes pour discuter de vos défis et comment je peux aider.",
    "value": "Je travaille avec des entreprises sur des projets de 2-6 mois pour réparer les systèmes de données cassés, réduire les coûts et améliorer la qualité. Basé à Paris & Montréal, disponible pour travail à distance et sur site.",
    "availability": "Disponible immédiatement pour nouveaux projets",
    "pricingTitle": "Tarification",
    "pricingStandard": "€1,200/jour pour projets standard d'ingénierie de données",
    "pricingComplex": "€1,500-1,800/jour pour travail d'architecture complexe",
    "pricingProject": "Tarification par projet disponible pour engagements multi-mois",
    "processTitle": "Comment Je Travaille",
    "processStep1": "1. Appel Découverte (30 min gratuit) - J'analyse vos principaux facteurs de coût et problèmes de qualité",
    "processStep2": "2. Proposition Personnalisée - Solution axée ROI avec timeline et tarification (pas de modèles génériques)",
    "processStep3": "3. Lancement Rapide - Démarrer le travail dans 1-2 semaines (je bouge vite)",
    "processStep4": "4. Mises à Jour Hebdomadaires - Métriques transparentes montrant progrès et valeur livrée",
    "service1": "Réparer les systèmes de données cassés",
    "service2": "Migrations sans interruption",
    "service3": "Frameworks qualité des données",
          "primaryCta": "Réserver Appel Stratégique Gratuit",
          "emailCta": "M'Envoyer un Email",
          "emailSubject": "Demande de Conseil",
          "rates": "€1,200-1,800/jour",
          "email": "Email",
          "location": "Localisation",
          "locationText": "Paris et Montréal",
          "linkedin": "LinkedIn"
  },
  "faq": {
    "title": "Questions Fréquemment Posées",
    "subtitle": "Questions courantes sur le travail avec moi",
    "ctaText": "Encore des questions ? Parlons-en.",
    "ctaButton": "Me Contacter",
    "items": [
      {
        "question": "Et si je ne suis pas prêt pour un engagement de 2-6 mois ?",
        "answer": "Je suis flexible. J'offre des évaluations plus courtes de 2-4 semaines et des quick wins de 1-2 mois pour problèmes urgents. On peut commencer petit (évaluer votre architecture de données et identifier les 3 principaux facteurs de coût) et augmenter selon les résultats. Beaucoup de clients commencent par une évaluation de 4 semaines puis étendent vers un engagement complet une fois qu'ils voient le ROI."
      },
      {
        "question": "Pouvez-vous travailler avec notre équipe existante ?",
        "answer": "Absolument. Je m'intègre parfaitement avec les équipes existantes. En fait, je préfère travailler aux côtés de vos ingénieurs car ils connaissent mieux vos systèmes. Je me concentre sur le transfert de connaissances, pour que votre équipe puisse maintenir et étendre les solutions après mon départ. À la Banque Nationale, j'ai dirigé une équipe de 7 ingénieurs, et les solutions fonctionnent encore avec succès aujourd'hui."
      },
      {
        "question": "Et si nous n'avons besoin d'aide que pour 1-2 semaines ?",
        "answer": "J'offre des sprints de conseil ciblés de 1-2 semaines pour problèmes spécifiques comme 'réparer notre pipeline de qualité des données' ou 'optimiser nos coûts Snowflake.' Parfaits pour problèmes urgents. Cependant, les améliorations d'architecture de données livrent généralement le meilleur ROI sur 2-6 mois car nous pouvons implémenter des solutions complètes, pas juste des correctifs rapides."
      },
      {
        "question": "Comment garantissez-vous le ROI sur projets de données ?",
        "answer": "Je commence chaque projet en identifiant métriques mesurables : objectifs de réduction d'erreurs, objectifs d'économies de coûts, améliorations d'efficacité. À la Banque Nationale, j'ai livré €200K+ de valeur annuelle en suivant KPIs spécifiques (30% réduction d'erreurs, 25% gain d'efficacité, 40% tests plus rapides). Vous recevrez mises à jour hebdomadaires avec ces métriques pour voir progrès en temps réel, pas juste à la fin."
      },
      {
        "question": "Travaillez-vous à distance ou sur site ?",
        "answer": "Les deux. Je suis basé à Paris et Montréal, mais je travaille à distance avec clients mondiaux (Europe, Amérique du Nord). Pour phases critiques ou workshops d'équipe, je peux voyager sur site. La plupart de mon travail est à distance, ce qui réduit coûts et permet planification flexible à travers fuseaux horaires."
      },
      {
        "question": "Qu'est-ce qui vous différencie d'autres consultants en données ?",
        "answer": "Trois choses : (1) Je me concentre sur ROI, pas juste implémentation technique. Chaque solution que je construis lie directement à économies de coûts ou améliorations de qualité mesurables. (2) Je bouge vite—je commence dans 1-2 semaines et livre résultats rapidement. (3) Je construis solutions que votre équipe peut maintenir. Pas de verrouillage fournisseur ou boîtes noires mystérieuses. Vous possédez le code et les connaissances."
      },
      {
        "question": "Et si le projet ne livre pas le ROI attendu ?",
        "answer": "Je suis transparent sur risques en amont. Pendant l'appel découverte gratuit, je vous dirai si je peux réalistement atteindre vos objectifs. Je ne prends pas projets où je ne peux pas livrer valeur. Si on découvre pendant l'engagement que la portée doit changer, on ajuste le plan transparentement. Je fixe attentes réalistes et livre résultats mesurables, c'est pourquoi clients continuent travailler avec moi."
      }
    ]
  },
  "footer": {
    "description": "Je répare les systèmes de données cassés. Plus de €200K de valeur annuelle livrés à la Banque Nationale. Disponible pour des projets de 2 à 6 mois.",
    "navigationTitle": "Navigation",
    "connectTitle": "Contact",
    "copyright": "© 2025 Fallou TALL. Tous droits réservés.",
    "availability": "Disponible immédiatement pour nouveaux projets"
  },
  "work": {
    "title": "Résultats Réels, Impact Réel",
    "subtitle": "Preuve concrète que je peux livrer un ROI mesurable pour votre entreprise",
    "businessImpact": "Preuve claire de valeur business et ROI à travers des résultats mesurables",
    "technicalInnovation": "Innovation technique combinée à un impact business mesurable",
    "cta": {
      "question": "Vous voulez des résultats similaires pour vos systèmes de données ?",
      "button": "Réserver un Appel Stratégique Gratuit"
    },
    "sections": {
      "keyAchievements": "Réalisations Clés",
      "keyResponsibilities": "Responsabilités Clés",
      "businessImpact": "Impact Business",
      "technicalInnovation": "Innovation Technique",
      "technologies": "Technologies & Outils"
    },
    "projects": {
      "sopra-steria-data-architect": {
        "company": "Sopra Steria",
        "role": "Architecte DATA Transverse - DCoE",
        "duration": "À partir de 11/2025 (À venir)",
        "location": "Courbevoie, France",
        "durationNote": "Poste à venir à partir de novembre 2025 - Disponible pour des projets de conseil jusqu'à cette date",
        "focusArea": "Architecture de transformation Data et IA multi-industries",
        "description": "À partir de novembre 2025, je rejoindrai le Centre d'Excellence Data de Sopra Steria en tant qu'Architecte de Données Transverse. Je concevrai des architectures de données pour des clients dans les secteurs Banque, Assurance, Secteur Public, Télécom, Média, Jeux, Industrie et Services. J'appuierai les équipes de livraison de la prospection à l'avant-vente jusqu'à la mise en production de solutions, en me concentrant sur l'architecture de données, la gouvernance, le catalogue de données, la gestion de référentiels, la qualité des données et l'industrialisation de l'IA. Disponible pour des projets de conseil jusqu'en novembre 2025.",
        "ctaQuestion": "Besoin d'architecture de données d'entreprise ? Disponible pour du conseil jusqu'en novembre 2025.",
        "keyResponsibilities": [
          "Concevoir des architectures optimisées (plateformes analytiques, temps réel, IoT)",
          "Concevoir et mettre en place des stratégies d'ingestion de données (temps réel, asynchrone, outils d'intermédiation)",
          "Participer à des ateliers clients de conception et de conseil",
          "Cadrage et avant-vente : analyser des appels d'offres, construire des réponses techniques (architecture fonctionnelle, technique et logicielle), présenter aux clients",
          "Encadrer des équipes de développement et participer à la livraison de solutions",
          "Collaborer avec les éditeurs leaders du marché et les hyperscalers"
        ],
        "technologies": ["Azure", "AWS", "Databricks", "Python", "Snowflake", "Spark", "SQL", "Java", "Scala", "MDM", "ETL", "ESB", "API Management", "Terraform"],
        "technicalInnovation": [
          "Patterns d'architecture de données multi-secteurs",
          "Frameworks de gouvernance et qualité des données",
          "Solutions de catalogue de données et gestion de référentiels",
          "Méthodologies d'industrialisation d'IA",
          "Architectures hybrides Cloud et on-premise",
          "Stratégies d'ingestion de données temps réel et batch"
        ],
        "industry": "Technologie & Conseil"
      },
      "onepoint-expert-data": {
        "company": "Onepoint",
        "role": "Consultant Expert Data",
        "duration": "09/2024 - 10/2025",
        "location": "Canada et France",
        "durationNote": "Temps partiel 09/2024-04/2025 en parallèle de la BNC, puis contrat complet jusqu'en 10/2025",
        "focusArea": "Implémentation de plateformes de données modernes et évolutives",
        "description": "J'ai aidé 4 clients entreprises au Canada et en France à résoudre leurs problèmes de plateforme de données. J'ai livré des audits techniques identifiant des opportunités d'économies, conçu des architectures modernes améliorant les performances de 40%, et établi des frameworks de gouvernance. J'ai atteint 95% de satisfaction client avec des résultats mesurables.",
        "ctaQuestion": "Vous souhaitez 95% de satisfaction client pour votre plateforme de données ?",
        "keyAchievements": [
          "Audits techniques, conception d'architecture de données",
          "Développement de pipelines ETL/ELT",
          "Implémentation DataOps",
          "Solutions de monitoring et sécurité des données",
          "Formation et accompagnement"
        ],
        "technologies": ["Azure", "AWS", "Databricks", "Python", "Snowflake", "Spark", "SQL", "Github Actions"],
        "businessImpact": [
          {
            "metricName": "Satisfaction Client",
            "percentage": 95,
            "description": "Haute satisfaction client avec les implémentations de plateformes de données"
          },
          {
            "metricName": "Performance Plateforme",
            "percentage": 40,
            "description": "40% d'amélioration des performances de traitement de données sur les plateformes clients grâce à l'optimisation de l'architecture"
          }
        ],
        "technicalInnovation": [
          "Patterns d'architecture de plateforme de données moderne",
          "Frameworks d'automatisation DataOps",
          "Solutions de gouvernance de données multi-cloud",
          "Conception d'architecture de données security-first"
        ],
        "industry": "Conseil & Technologie"
      },
      "bnc-analytical-foundation": {
        "company": "Banque Nationale du Canada",
        "role": "Lead Data Engineer",
        "duration": "11/2021 - 04/2025",
        "location": "Montréal, Canada",
        "focusArea": "Entrepôt de données moderne permettant des reporting critiques et des analytiques à l'échelle de la banque",
        "description": "J'ai construit un entrepôt de données moderne (data lake + data warehouse) qui a permis des décisions d'affaires critiques pour la 6ème plus grande banque du Canada. La plateforme alimente la stratégie d'acquisition client du Marketing (suivi CAC, segmentation, Next Best Action), la détection de fraude en temps réel, les analytiques Cartes & Paiements, et le reporting exécutif utilisé par la direction. Initialement construit sur Azure avec Snowflake et Databricks, puis migré avec succès vers AWS sans interruption. Le framework de qualité des données PySpark que j'ai développé a réduit les erreurs de 30% et a été adopté à l'échelle de la banque par plus de 10-20 équipes (Marketing, Gestion de Patrimoine, Cartes, Fraude). Collaboration avec le CDO pour établir des standards de gouvernance de la qualité des données à l'échelle de la banque.",
        "ctaQuestion": "Vous souhaitez plus de €200K de valeur annuelle de vos systèmes de données ?",
        "keyAchievements": [
          "Direction d'une équipe de 7 ingénieurs construisant un entrepôt de données moderne (Snowflake + Databricks) sur AWS et Azure, initialement sur Azure puis migré vers AWS",
          "Construction d'une fondation analytique bancaire permettant des reporting critiques : Marketing (CAC, segmentation, Next Best Action), Cartes & Paiements, Détection de Fraude, Reporting Exécutif, IA/ML",
          "Conception d'architecture et de modèles de données pour une plateforme analytique multi-domaines supportant des millions d'enregistrements clients",
          "Développement d'un framework de qualité des données PySpark atteignant 30% de réduction d'erreurs dans les pipelines de l'équipe ; outil adopté à l'échelle de la banque par plus de 10-20 équipes à travers Marketing, Gestion de Patrimoine, Cartes, Fraude",
          "Augmentation de l'efficacité de l'équipe de 25% (3 heures/jour libérées par ingénieur) et réduction du temps de test de 40%",
          "Travail avec le CDO pour définir, standardiser et opérationnaliser les dimensions de qualité des données, permettant une cohérence inter-départementale dans les standards de qualité"
        ],
        "technologies": ["Azure", "AWS", "Snowflake", "Databricks", "Python", "PySpark", "Spark", "SQL", "Terraform"],
        "businessImpact": [
          {
            "metricName": "Réduction d'Erreurs",
            "percentage": 30,
            "description": "30% de réduction des erreurs de traitement dans les pipelines de production de l'équipe grâce au framework PySpark (plus de €65K d'économies annuelles)"
          },
          {
            "metricName": "Amélioration Efficacité",
            "percentage": 25,
            "description": "25% d'augmentation de l'efficacité de l'équipe, libérant 3 heures/jour par ingénieur pour un travail à haute valeur ajoutée (plus de €65K de valeur annuelle)"
          },
          {
            "metricName": "Vitesse Tests",
            "percentage": 40,
            "description": "40% de tests QA plus rapides grâce à l'automatisation, réduisant les cycles de mise en production de jours à heures (plus de €20K de valeur annuelle)"
          },
          {
            "metricName": "Adoption Banque",
            "percentage": 100,
            "description": "Framework de qualité des données adopté comme standard bancaire par Marketing, Gestion de Patrimoine, Cartes, Fraude, et plus de 10-20 équipes dans l'organisation"
          }
        ],
        "technicalInnovation": [
          "Architecture d'entrepôt de données moderne (data lake + data warehouse) avec Snowflake et Databricks",
          "Stratégie de migration multi-cloud (Azure vers AWS) sans interruption",
          "Framework de qualité des données PySpark adopté à l'échelle de la banque",
          "Framework de gouvernance de qualité des données inter-départementale",
          "Architecture analytique évolutive supportant des millions d'enregistrements clients",
          "Modélisation de données pour analytiques multi-domaines (Marketing, Cartes, Fraude, Exécutif, IA/ML)"
        ],
        "industry": "Banque & Services Financiers"
      },
      "orange-hadoop-migration": {
        "company": "Orange Côte d'Ivoire",
        "role": "Lead Data Engineer",
        "duration": "09/2020 - 08/2021",
        "location": "Côte d'Ivoire",
        "focusArea": "Migration et mise à jour de cluster Hadoop",
        "description": "J'ai dirigé la migration sans interruption des flux de données vers un nouveau cluster Hadoop. J'ai développé des outils de productivité et orchestré des pipelines de données complexes.",
        "ctaQuestion": "Besoin de migration sans interruption pour vos systèmes de données ?",
        "keyAchievements": [
          "J'ai supervisé la migration des flux de données",
          "J'ai migré Flume/Pig/Spark1/Sqoop vers Spark2",
          "J'ai développé une bibliothèque Spark atteignant 20% d'augmentation de productivité"
        ],
        "technologies": ["Hadoop", "Scala", "Spark", "Oracle"],
        "businessImpact": [
          {
            "metricName": "Amélioration Productivité",
            "percentage": 20,
            "description": "20% d'augmentation de la productivité de l'équipe d'ingénierie de données grâce à l'adoption de la bibliothèque Spark personnalisée"
          },
          {
            "metricName": "Succès Migration",
            "percentage": 100,
            "description": "Migration complète sans interruption des flux de données critiques"
          }
        ],
        "technicalInnovation": [
          "Stratégie de migration sans interruption",
          "Bibliothèque Spark de productivité personnalisée",
          "Orchestration de workflow avancée",
          "Modernisation de systèmes legacy"
        ],
        "industry": "Télécommunications"
      },
      "orange-qoe-management": {
        "company": "Orange Sénégal",
        "role": "Senior Data Engineer & Data Scientist",
        "duration": "06/2019 - 08/2020",
        "location": "Sénégal",
        "focusArea": "Gestion de l'Expérience Client",
        "description": "J'ai développé des modèles ML pour la gestion de l'expérience client et la prédiction de QoE.",
        "keyAchievements": [
          "J'ai développé des modèles ML prédisant la QoE client avec 15% d'amélioration de satisfaction",
          "J'ai automatisé les processus réduisant le temps de traitement de 30%",
          "Analyse de corrélation NPS-QoE avec tableaux de bord Tableau"
        ],
        "technologies": ["Hadoop", "Scala", "Spark", "SQL Server", "Tableau", "Python", "Scikit-Learn", "Flask"],
        "businessImpact": [
          {
            "metricName": "Satisfaction Client",
            "percentage": 15,
            "description": "15% d'amélioration des scores de satisfaction client grâce à la prédiction proactive de QoE et l'intervention"
          },
          {
            "metricName": "Efficacité Traitement",
            "percentage": 30,
            "description": "30% de réduction du temps de traitement d'analytics grâce à l'automatisation des flux de travail"
          }
        ],
        "technicalInnovation": [
          "Modèles ML avancés pour prédiction QoE",
          "Pipelines de traitement de données automatisés",
          "Analytics client en temps réel",
          "Analyse de corrélation avancée"
        ],
        "industry": "Télécommunications"
      },
      "orange-data-monetization": {
        "company": "Orange Sénégal",
        "role": "Data Engineer & Data Scientist",
        "duration": "10/2018 - 04/2019",
        "location": "Sénégal",
        "focusArea": "Plateforme de monétisation de données numériques",
        "description": "Architecture micro-services avec Kubernetes/Kafka/Cassandra/Spark/Node.js, modélisation de localisation à partir de données CDR, prédiction de mouvement de population avec SparkML.",
        "keyAchievements": [
          "J'ai développé une architecture micro-services avec Kubernetes/Kafka/Cassandra/Spark/Node.js",
          "Modélisation de localisation à partir de données CDR",
          "Prédiction de mouvement de population avec SparkML"
        ],
        "technologies": ["Cassandra", "Kafka", "Kubernetes", "Scala", "Spark"],
        "businessImpact": [
          {
            "metricName": "Évolutivité Plateforme",
            "percentage": 300,
            "description": "Atteint 3x d'évolutivité avec architecture micro-services supportant le traitement de données haute volumétrie"
          },
          {
            "metricName": "Monétisation Données",
            "percentage": 25,
            "description": "Permis de nouveaux flux de revenus grâce aux insights basés sur les données"
          }
        ],
        "technicalInnovation": [
          "Architecture de données micro-services",
          "Analytics de localisation en temps réel",
          "Modèles de prédiction de mouvement de population",
          "Traitement avancé de données CDR"
        ],
        "industry": "Télécommunications"
      }
    }
  },
  "stats": {
    "yearsExperience": "Années d'Expérience",
    "technologiesMastered": "Technologies Maîtrisées",
    "enterpriseProjects": "Projets d'Entreprise"
  },
  "common": {
    "loading": "Chargement...",
    "error": "Une erreur s'est produite",
    "retry": "Réessayer",
    "close": "Fermer",
    "open": "Ouvrir",
    "readMore": "Lire la suite",
    "viewProject": "Voir le projet",
    "learnMore": "En savoir plus",
    "pageNotFound": "Page non trouvée",
    "pageNotFoundDescription": "Désolé, la page que vous recherchez n'existe pas ou a été déplacée.",
    "goHome": "Retour à l'accueil",
    "goBack": "Retour",
    "tryThesePages": "Ou essayez une de ces pages :"
  },
  "workedWith": {
    "title": "Ils Me Font Confiance",
    "subtitle": "Résultats prouvés auprès d'organisations d'entreprise",
    "industries": {
      "banking": "Banque",
      "telecommunications": "Télécommunications",
      "consulting": "Conseil",
      "technology": "Technologie"
    },
    "industryPills": {
      "bankingFinance": "Banque & Finance",
      "telecommunications": "Télécommunications",
      "consulting": "Conseil",
      "technology": "Technologie"
    }
  }
}
